{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Aula Pré processamento de texto"
      ],
      "metadata": {
        "id": "Wk3Mw1EiQbWa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikr4ZLjDUUXO",
        "outputId": "3c64d885-1308-41dc-d835-c3729233e96a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.3)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download pt_core_news_sm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "en0OMey7VAb_",
        "outputId": "80ad0bad-a83c-496d-d0f0-f1637e4d8e5b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pt-core-news-sm==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-3.7.0/pt_core_news_sm-3.7.0-py3-none-any.whl (13.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from pt-core-news-sm==3.7.0) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (13.9.3)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.1.2)\n",
            "Installing collected packages: pt-core-news-sm\n",
            "Successfully installed pt-core-news-sm-3.7.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('pt_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Unidecode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Obs3Bg29TJmG",
        "outputId": "3db95679-35a7-48fe-d499-2ec4f9228ec0"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Unidecode in /usr/local/lib/python3.10/dist-packages (1.3.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texto = \"Olá, mundo! Bem-vindos ao processamento de linguagem natural.\""
      ],
      "metadata": {
        "id": "aVxOhBEmRnjf"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(texto)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNsMfbOtKWz3",
        "outputId": "6396ce8c-5a46-4132-c1ba-964b9efa6693"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Olá, mundo! Bem-vindos ao processamento de linguagem natural.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remoção de pontuação, elimina caracteres que não possuem relevância para o seu problema em questão"
      ],
      "metadata": {
        "id": "JaUNw9xNQWLz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aiJ8GQ3XPu4C",
        "outputId": "6f923d80-50d7-49ad-db72-7288e695dbb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sss\n"
          ]
        }
      ],
      "source": [
        "import re # importa a biblioteca de expressões regulares\n",
        "# A expressão r'[^ws]' significa que qualquer caractere que não seja um espaço em branco (w) ou uma letra 's' será removido do texto\n",
        "texto = re.sub(r'[^ws]', '', texto) # usa re.sub para substituir partes do texto baseado em uma expressão regular\n",
        "print(texto)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remoção de acentuação, para homogeneizar o texto, precisa remover a acentuação, dessa forma, se houver variações no texto, não teremos problema\n",
        "\n",
        "Biblioteca Unicode, converte caracteres em seus equivalentes mais próximos em ASCII\n",
        "\n",
        "É simples porém pode não lidar com todos os caracteres de todas as línguas perfeitamente"
      ],
      "metadata": {
        "id": "NRT0IEvPR-wP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unidecode import unidecode\n",
        "texto = unidecode(texto)\n",
        "print(texto)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sha2XhdvSrtq",
        "outputId": "7d6ef7ba-14cd-4afa-fc38-3e8f7a3b884f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ola, mundo! Bem-vindos ao processamento de linguagem natural.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remoção de stopwords, remove palavras comuns que não contribuem para a análise (como \"a\", \"e\",\"o\")\n",
        "\n",
        "biblioteca nltk"
      ],
      "metadata": {
        "id": "4tp1c7lhSBlp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "tokens = word_tokenize(texto)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ysg6_FZ4R7H_",
        "outputId": "29888e14-d939-4706-81ce-b74374ceb3b7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Ola', ',', 'mundo', '!', 'Bem-vindos', 'ao', 'processamento', 'de', 'linguagem', 'natural', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "stop_words = set(stopwords.words('portuguese'))\n",
        "print(stop_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-2oNksnRBvy",
        "outputId": "bb816a66-5585-4772-f922-87c42f6b3b70"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'tivessem', 'deles', 'com', 'éramos', 'meu', 'isso', 'dela', 'esteja', 'estas', 'houver', 'seria', 'somos', 'aquela', 'de', 'teríamos', 'no', 'este', 'tenho', 'houveremos', 'também', 'nosso', 'tiver', 'elas', 'houveríamos', 'quem', 'esta', 'sem', 'tu', 'os', 'seu', 'o', 'houveriam', 'houvéramos', 'aquele', 'estar', 'estou', 'hajam', 'hei', 'lhe', 'aqueles', 'estamos', 'tivemos', 'aos', 'serei', 'quando', 'tenhamos', 'como', 'sua', 'que', 'dos', 'fôssemos', 'terão', 'tínhamos', 'estivermos', 'e', 'haja', 'eu', 'nos', 'seremos', 'ele', 'fui', 'essa', 'serão', 'fosse', 'muito', 'houveria', 'sejam', 'nas', 'vos', 'for', 'pelas', 'teus', 'delas', 'num', 'pelos', 'foram', 'às', 'nós', 'terá', 'sou', 'tiverem', 'estava', 'temos', 'terei', 'houverei', 'estiverem', 'houve', 'sejamos', 'estávamos', 'ao', 'pela', 'tinha', 'tivéssemos', 'tinham', 'da', 'minha', 'você', 'formos', 'entre', 'ser', 'havemos', 'forem', 'houvéssemos', 'um', 'estiver', 'essas', 'aquelas', 'me', 'esteve', 'não', 'qual', 'suas', 'estivera', 'tua', 'houverá', 'esses', 'fora', 'a', 'tém', 'pelo', 'estão', 'ela', 'tive', 'estes', 'uma', 'à', 'em', 'esse', 'estavam', 'houvera', 'seríamos', 'tivéramos', 'estivemos', 'seus', 'para', 'por', 'tivera', 'eles', 'teve', 'tenham', 'houverem', 'vocês', 'teriam', 'nossos', 'teu', 'se', 'será', 'até', 'mais', 'eram', 'estivéramos', 'nossa', 'foi', 'era', 'são', 'minhas', 'estivesse', 'está', 'houvemos', 'já', 'seriam', 'tivesse', 'das', 'é', 'nem', 'tivermos', 'depois', 'isto', 'as', 'hão', 'estivéssemos', 'estive', 'lhes', 'houveram', 'fôramos', 'na', 'estivessem', 'ou', 'houvermos', 'tenha', 'do', 'estejam', 'teremos', 'mas', 'mesmo', 'numa', 'estiveram', 'aquilo', 'houvessem', 'tiveram', 'há', 'fomos', 'nossas', 'te', 'teria', 'dele', 'houverão', 'estejamos', 'haver', 'hajamos', 'seja', 'tuas', 'fossem', 'meus', 'tem', 'houvesse', 'só'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "já existe vários stopwords mapeadas dentro da biblioteza nltk mas é possível estender ela para nosso caso add manualmente mais palavras ou removendo"
      ],
      "metadata": {
        "id": "tw6vL_zERNwJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = word_tokenize(texto)\n",
        "valid_tokens = [token for token in tokens if token not in stop_words]\n",
        "texto = ' '.join(valid_tokens)\n",
        "print(texto)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evX4B32jRMA4",
        "outputId": "90cfe413-8f61-48bc-e3c9-ad0142c325fc"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ola , mundo ! Bem-vindos processamento linguagem natural .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming é o processo de simplificar as palavras, reduzindo-as às suas formas básicas, isso é feito removendo partes das palavras, como sufixo e prefixo, o resultado pode ser uma palavra que não é encontrada no dicionário mas sim uma forma simplificada\n",
        "\n",
        "biblioteca NLTK tem um stemmer chamado RSLPStemmer usado para lidar com a língua portuguesa"
      ],
      "metadata": {
        "id": "CtgcaIToSEmf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('rslp')\n",
        "from nltk.stem import RSLPStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "stemmer = RSLPStemmer()\n",
        "tokens = word_tokenize(texto)\n",
        "stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "texto = ' '.join(stemmed_tokens)\n",
        "print(texto)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8QFkN8ATb0P",
        "outputId": "e8222ddf-2bff-4690-8ce7-3e742d476fd6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ola , mund ! bem-v process lingu natur .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Unzipping stemmers/rslp.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lematização, reduz as palavras à sua forma canônica ou dicionária, oq geralmente produz resultados mais precisos e legíveis do que apenas cortar sufixos e prefixos como no stremming\n",
        "\n",
        "biblioteca spacy"
      ],
      "metadata": {
        "id": "Ori71jdQS8Kg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('pt_core_news_sm')\n",
        "doc = nlp(texto)\n",
        "token_lematizado = [token.lemma_ for token in doc]\n",
        "texto_lematizado = ' '.join(token_lematizado)\n",
        "print(texto_lematizado)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdn71wHdVKYk",
        "outputId": "40ac2b75-6316-46a6-d970-e9b880866787"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ola , mund ! bem-v process Lingu em atur .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correspondência fuzzy, para corrigir erros de digitação, variações ortográficas, abreviações e outros tipos de variações\n",
        "\n",
        "é muito usada para correção automática de palavras\n",
        "\n",
        "técnicas para a correspondência fuzzy\n",
        "- Similaridade de Jaro\n",
        "- Similaridade de Jaro-Winkler\n",
        "- Distância de Levenshtein\n",
        "- Distância de Damerau-Levenshtein"
      ],
      "metadata": {
        "id": "pA-6VvWMTUHN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similaridade de Jaro, o reultado do algoritmo do Jaro é um valor entre 0 e 1, em que 0 indica nenhuma similaridade e 1 indoca uma correspondência perfeita. Quanto maior o valor, maior a similaridade entre as duas strings\n",
        "\n",
        "Similaridade de Jaro-Winkler, este algoritmo estende a ao anterior add uma constante que favorece quando a comparação ocorre com textos que possuem equivalência longa no início\n",
        "\n",
        "Distância de Levenshtein, conhecida como distância de edição, é uma medida da quantidade mínima de operações necessárias para transformar uma string em outra. As operações permitidas são inserção, exclusão e substituição de um caractere. Assim, a Distância de Levenshtein entre duas strisngs é o número mínimo de operações necessáras para transformar uma string na outra\n",
        "\n",
        "\n",
        "Distância de Damerau-Levenshtein, estende a Distância de Levenshtein ao add a transposição adjacente entre as possíveis operações\n",
        "ex a distância entre as palavras \"flom\" e \"molf\" com transposição adjacente é 1, já que somente transpondo o \"m\" e o \"f\" consegue transformar uma palavra em outra\n",
        "na Distância de Levenshtein, teria o valor de 2 precisa de uma remoção e uma adição\n",
        "biblioteca jellyfish"
      ],
      "metadata": {
        "id": "lljEZM0jYqK5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jellyfish\n",
        "\n",
        "texto_original = \"aprendizado de máquina\"\n",
        "texto_erro = \"aprendizad d maquina\"\n",
        "texto_aleatorio = \"bom dia alunos FIAP\"\n",
        "\n",
        "similaridades = [\n",
        " \"jaro_similarity\",\n",
        " \"damerau_levenshtein_distance\",\n",
        " \"levenshtein_distance\",\n",
        " \"jaro_winkler_similarity\"\n",
        "]\n",
        "\n",
        "comparacoes = {\n",
        " 'erro_digitacao': [texto_original, texto_erro],\n",
        " 'comp_aleatoria': [texto_original, texto_aleatorio]\n",
        "}\n",
        "resultados = {}\n",
        "for distancia in similaridades:\n",
        "  resultados[distancia] = {\n",
        "    comp: getattr(jellyfish, distancia)(\n",
        "      comparacoes.get(comp)[0], comparacoes.get(comp)[1]\n",
        "    )\n",
        "    for comp in comparacoes\n",
        "}\n",
        "resultados"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sg--cwJucCyJ",
        "outputId": "948e481c-ce88-4dd7-cb05-1ac97a776461"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'jaro_similarity': {'erro_digitacao': 0.9027910685805423,\n",
              "  'comp_aleatoria': 0.5164805954279639},\n",
              " 'damerau_levenshtein_distance': {'erro_digitacao': 3, 'comp_aleatoria': 19},\n",
              " 'levenshtein_distance': {'erro_digitacao': 3, 'comp_aleatoria': 19},\n",
              " 'jaro_winkler_similarity': {'erro_digitacao': 0.9416746411483253,\n",
              "  'comp_aleatoria': 0.5164805954279639}}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "nesse exemplo da pra ver que a Distância de Damerau-Levenshtei e a Distância de Levenshtei deram os mesmos valores, visto que não tivemos transposição em nenhuma dass comparações. As diferenças foram de remoção (a letra \"o\" de \"aprendizado\" e a letra \"e\" de \"de\") ou substituição (á por a) na comparação de erro de digitação de erro de digitação e quase tudo na comparação aleatória"
      ],
      "metadata": {
        "id": "Td04gz5fcYrt"
      }
    }
  ]
}